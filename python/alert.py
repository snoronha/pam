"""Prediction and Alert module.

The :mod:`autogrid.pam.alert` module generated predictions and issues alerts
based on anomalies generated by the :mod:`autogrid.pam.anomaly` module.
"""

# Copyright (c) 2011-2015 AutoGrid Systems
# Author(s): 'Trevor Stephens' <trevor.stephens@auto-grid.com>


import numpy as np
import pandas as pd
from datetime import datetime
from pytz import timezone, UTC


HOUR = pd.Timedelta(1, 'h')
DAY = pd.Timedelta(days=1)
EASTERN = timezone('US/Eastern')


class _Alert(object):

    """A container for details about an alert.

    Parameters
    ----------
    feeder_id : str
        The feeder ID number.

    timestamp : pd.Timestamp
        The time of the alert.

    alert_type : str
        The label of the alert (Red, Orange, Yellow).

    model_id : str
        The ID for the model, used for reporting.

    anomalies : pd.DataFrame
        The anomalies that caused the alert.

    feeder_metadata : pd.DataFrame
        Human readable metadata about the feeder.
    """

    def __init__(self, feeder_id, timestamp, alert_type, model_id, anomalies,
                 feeder_metadata):
        self.feeder_id = feeder_id
        self.timestamp = timestamp
        self.alert_type = alert_type
        self.model_id = model_id
        self.anomalies = anomalies
        self.feeder_metadata = feeder_metadata

    def __str__(self):
        """Overload `print` output of the object to give summary of alert."""
        return '%s Alert: Feeder %s @ %s' % (self.alert_type,
                                             self.feeder_id,
                                             self.timestamp)

    def to_dict(self):
        """Generate a serializable representation of the alert.

        Returns
        -------
        alert_details : dict
            dict representing the alert. Can be serialized to JSON for sending
            to the customer.
        """
        cols = ['Anomaly', 'DeviceId', 'DevicePh', 'DeviceType', 'Signal', 'Time']

        utc_time = self.timestamp.strftime('%Y-%m-%dT%H:%M:%SZ')

        utc_sent = datetime.utcnow()
        utc_sent = UTC.localize(utc_sent).strftime("%Y-%m-%dT%H:%M:%SZ")

        alert_details = {'feeder_id': self.feeder_id,
                         'alert_type': self.alert_type,
                         'model_version': self.model_id,
                         'alert_time_utc': utc_time,
                         'alert_sent_utc': utc_sent}

        anomalies = self.anomalies[cols].copy()
        anomalies.Time = anomalies.Time.dt.strftime("%Y-%m-%dT%H:%M:%SZ")
        anomalies = anomalies.T.to_dict().values()
        alert_details['anomalies'] = anomalies

        return alert_details

    def to_html(self):
        """Generate a human-readable HTML report representation of the alert.

        Returns
        -------
        html : str
            Formatted HTML code representing the alerts. Can be rendered in
            emails for sending to the customer.
        """
        cols = ['Anomaly', 'DeviceId', 'DevicePh', 'DeviceType', 'Signal', 'Time']

        html = '<center>\n'

        dash_link = '<a href="http://dpdcapps/feeders/dashboardReport?feeder=' \
                    '%s&style=dash">%s</a>' % (self.feeder_id, self.feeder_id)

        alert = pd.DataFrame({'Value': ['dash_link',
                                        self.alert_type,
                                        self.model_id,
                                        self.timestamp.tz_convert(EASTERN)]},
                             index=['Feeder', 'Alert', 'Model Version', 'Time'])
        alert = alert.to_html(header=False, justify='center')
        alert = alert.replace('dash_link', dash_link)
        new_style = 'style="border-collapse:collapse;">\n' \
                    '  <caption>Alert Details</caption>'
        alert = alert.replace('class="dataframe">', new_style)
        alert = alert.replace('<th>', '<th bgcolor="#e0e0e0">')
        alert = alert.replace('<td>Red', '<td bgcolor="#F78181">Red')
        alert = alert.replace('<td>Orange', '<td bgcolor="#F7BE81">Orange')
        alert = alert.replace('<td>Yellow', '<td bgcolor="#F3F781">Yellow')
        html += alert
        html += '\n<BR>\n<BR>\n'

        metadata = pd.DataFrame(self.feeder_metadata).to_html(header=False,
                                                              justify='center')
        new_style = 'style="border-collapse:collapse;">\n' \
                    '  <caption>Feeder Details</caption>'
        metadata = metadata.replace('class="dataframe">', new_style)
        metadata = metadata.replace('<th>', '<th bgcolor="#e0e0e0">')
        html += metadata
        html += '\n<BR>\n<BR>\n'

        # Need to override pandas options to avoid clipping long strings
        old_option = pd.get_option('display.max_colwidth')
        pd.set_option('display.max_colwidth', -1)
        anomalies = self.anomalies.copy().sort_values(by=['Time', 'Anomaly'])
        anomalies.Time = anomalies.Time.dt.tz_convert(EASTERN)
        anomalies = anomalies.to_html(index=False, columns=cols,
                                      justify='center')
        pd.set_option('display.max_colwidth', old_option)
        new_style = 'style="border-collapse:collapse;">\n' \
                    '  <caption>Alarm Details</caption>'
        anomalies = anomalies.replace('class="dataframe">', new_style)
        anomalies = anomalies.replace('<thead>', '<thead bgcolor="#e0e0e0">')
        html += anomalies
        html += '\n<BR>\n<BR>\nPowered by AutoGrid\n'
        html += '</center>'

        html = html.replace('-05:00', ' EST').replace('-04:00', ' EDT')
        html = html.replace('-0500', ' EST').replace('-0400', ' EDT')

        return html


class AlertGenerator(object):

    """Generates alerts from anomalies.

    Given a set of anomalies, this module determines if an alert has been
    triggered and can then create a formatted alert.

    Parameters
    ----------
    clf : a fitted scikit-learn classification model
        The classifier used to generate predictions.

    dataset_config : dict
        The configuration of the dataset. For example, please see
        ``pam_1_0_dataset.yml``.

    anomaly_map : dict
        The mapping of anomaly names from the :mod:`autogrid.pam.anomaly` module
        to those listed in `dataset_config`. Used to combine similar anomalies
        that come from different sources. For example, please see
        ``pam_1_0_anomaly_map.yml``.

    alert_config : dict
        The thresholds of the alert levels. For example, please see
        ``pam_1_0_alert.yml``.

    model_id : str
        The ID for the model, used for reporting.

    feeder_df : Pandas DataFrame
        A DataFrame with an index column of feeder IDs, and a column named
        "CUSTOMERS" with a count of the number of customers on the feeder. Also
        columns for each "constant" or "duration" variables required.

    start_time : datetime object, optional (default=None)
        The earliest time to report an anomaly for. If not provided, the
        earliest available data will be used.

    end_time : datetime object, optional (default=None)
        The latest time to report an anomaly for. If not provided, the oldest
        available data will be used.

    Attributes
    ----------
    feeder_ignore : set
        Feeder IDs that will be ignored based on having either zero length or
        zero customers.

    X : Pandas DataFrame
        A DataFrame with the anomaly signatures. Must be in same column
        order as the DataFrame used to fit the ``clf``.

    y : Pandas DataFrame
        A DataFrame with the Feeder ID and timestamp associated with each
        signature in ``X``. It is in same row order as ``X``.

    preds : Pandas DataFrame
        A DataFrame with the Feeder ID and timestamp associated with each
        signature in ``X`` as well as the probability of failure and the alert
        level. It is in same row order as ``X``.

    alerts : list of Alert objects
        Each element in alerts corresponds to a single alert to be sent.
    """

    def __init__(self, clf, dataset_config, anomaly_map, alert_config,
                 model_id, feeder_df, start_time=None, end_time=None):

        self.clf = clf
        self.dataset_config = dataset_config
        self.anomaly_map = anomaly_map
        self.alert_config = alert_config
        self.model_id = model_id
        self.feeder_df = feeder_df
        self.start_time = start_time
        self.end_time = end_time

        low_cust = feeder_df.loc[feeder_df.CUSTOMERS < 100].index.values
        zero_len = feeder_df.loc[(feeder_df.FDR_OH == 0) &
                                 (feeder_df.FDR_UG == 0)].index.values
        self.feeder_ignore = set(list(low_cust) + list(zero_len))

    def _clean_data(self, anomalies, feeders=None):
        """Private function to clean up anomalies dataset.

        Parameters
        ----------
        anomalies : pd.DataFrame
            DataFrame containing one row per anomaly. Same format as the output
            of the anomaly module.

        feeders : list or set
            Only specified for alert output, should not be used for prediction.
        """
        def strip_seconds(t):
            """Reduce granularity of timestamps to 1 minute."""
            return t.replace(second=0, microsecond=0)

        anomalies.Time = [strip_seconds(t) for t in anomalies.Time]
        # Remove zero-length or low-customer feeders
        anomalies = anomalies.loc[-anomalies.Feeder.isin(self.feeder_ignore)]
        # Remove feeders without available meta-data
        good_feeders = self.feeder_df.index.values
        anomalies = anomalies.loc[anomalies.Feeder.isin(good_feeders)]

        # Rename anomalies for use with configuration file
        good_anomalies = self.anomaly_map.keys()
        anomalies = anomalies.loc[anomalies.Anomaly.isin(good_anomalies)]
        anomalies.Anomaly = anomalies.Anomaly.map(self.anomaly_map)

        if self.end_time is not None:
            anomalies = anomalies.loc[anomalies.Time < self.end_time]

        # Further screening for signature processing
        if feeders is None:

            # Drop duplicates where required
            drop_dupes = [c['lookup'] for c in self.dataset_config if
                          'keep_all' in c and c['keep_all'] is False]
            drop_dupes = anomalies.loc[anomalies.Anomaly.isin(drop_dupes)]
            drop_dupes = drop_dupes.drop_duplicates(['Anomaly', 'Feeder', 'Time'])
            keep_dupes = [c['lookup'] for c in self.dataset_config if
                          'keep_all' in c and c['keep_all'] is True]
            keep_dupes = anomalies.loc[anomalies.Anomaly.isin(keep_dupes)]

            anomalies = []
            if not drop_dupes.empty:
                anomalies.append(drop_dupes)
            if not keep_dupes.empty:
                anomalies.append(keep_dupes)
            if anomalies:
                anomalies = pd.concat(anomalies)
            else:
                anomalies = pd.DataFrame()

        else:
            # Remove feeders that are not required for alert report
            anomalies = anomalies.loc[anomalies.Feeder.isin(feeders)]

        anomalies = anomalies.dropna()

        return anomalies

    @staticmethod
    def _triple_threat(anomalies):
        """Private function to generate the triple_threat rule.

        Parameters
        ----------
        anomalies : pd.DataFrame
            DataFrame containing one row per anomaly. Same format as the output
            of the anomaly module.
        """
        # Note: Must be a cleaned dataset
        triple_threat = ['PF_SPIKES', 'THD_SPIKES', 'ZERO_CURRENT',
                         'ZERO_POWER', 'ZERO_VOLTAGE']
        triple_threat = anomalies.loc[anomalies.Anomaly.isin(triple_threat)]
        cols = ['Anomaly', 'Feeder', 'Time']
        triple_threat = triple_threat.drop_duplicates(cols)
        cols = ['Feeder', 'Time']
        triple_threat = triple_threat.groupby(cols)['Anomaly'].count()
        triple_threat = triple_threat[triple_threat > 2]
        triple_threat = triple_threat.reset_index()[cols]

        return triple_threat

    def _transform(self, unprocessed, processed):
        """Private function to compute anomalies for the specified feeder.

        Parameters
        ----------
        unprocessed : pd.DataFrame
            DataFrame containing one row per anomaly. Same format as the output
            of the anomaly module. These are the new, previously unprocessed
            anomalies.

        processed : pd.DataFrame, or None
            DataFrame containing one row per anomaly. Same format as the output
            of the anomaly module. These are the previously processed anomalies
            from older evaluation jobs. Use None or an empty DataFrame to
            represent the case of turning on the system where there is no old
            previously processed data.
        """
        # Collate & clean anomalies
        use_processed = True
        if processed is None or processed.empty:
            use_processed = False
        if use_processed:
            processed = self._clean_data(processed.copy())
            if processed.empty:
                use_processed = False
        unprocessed = self._clean_data(unprocessed.copy())

        # Create empty datasets
        y = {'FEEDER': [], 'TIMESTAMP': []}
        col_order = [c['name'] for c in self.dataset_config]
        X = {c: [] for c in col_order}

        triggers = [c['lookup'] for c in self.dataset_config if
                    c['type'] == 'trigger']
        durations = [c['name'] for c in self.dataset_config if
                     c['type'] == 'duration']
        constants = [c['name'] for c in self.dataset_config if
                     c['type'] == 'constant']

        if not unprocessed.empty:
            for f in unprocessed.Feeder.unique():

                f_anoms = unprocessed.loc[unprocessed.Feeder == f]
                if use_processed:
                    f_hist = processed.loc[processed.Feeder == f]
                    if not f_hist.empty:
                        f_anoms = pd.concat([f_anoms, f_hist])

                # AMI last gasps / power downs can come in slowly over time.
                # This results in duplicate anomalies that need to be dropped.
                # Sorting by "Signal" will keep the highest anomaly only.
                f_anoms = f_anoms.sort_values('Signal', ascending=False)
                dup_cols = ['Anomaly', 'Feeder', 'Time']
                f_anoms = f_anoms.loc[(f_anoms.DeviceType != 'AMI') |
                                      (~f_anoms[dup_cols].duplicated())]

                # Only create new rows for "triggers"
                row_times = f_anoms.loc[f_anoms.Anomaly.isin(triggers)]
                row_times = np.unique(row_times.Time.tolist())
                if self.start_time is not None:
                    row_times = row_times[row_times >= self.start_time]
                if self.end_time is not None:
                    row_times = row_times[row_times < self.end_time]

                for t in row_times:

                    y['FEEDER'].append(f)
                    y['TIMESTAMP'].append(t)

                    for col in self.dataset_config:

                        # These variables' utility was found to be
                        # questionable, and so are manually set to all possible
                        # combinations of their boolean values. We set to zero
                        # for now, and manually override at prediction time.
                        deprecated = ['FDR_GEO_0', 'FDR_GEO_1', 'FDR_GEO_2',
                                      'IS_DADE', 'SIG_OUTLIER']

                        if col['type'] in {'trigger', 'flag', 'background',
                                           'cluster', 'sequence'}:
                            # These require lookups on the anomaly table
                            c_name = col['lookup']
                            min_lag = t - pd.Timedelta(hours=col['min_lag'])
                            max_lag = t - pd.Timedelta(hours=col['max_lag'])

                            c_anoms = f_anoms.loc[(f_anoms.Anomaly == c_name) &
                                                  (f_anoms.Time <= min_lag) &
                                                  (f_anoms.Time > max_lag)]

                            if c_anoms.empty:
                                X[col['name']].append(0)
                                continue
                            if col['type'] in {'trigger', 'background'}:
                                X[col['name']].append(c_anoms.shape[0])
                            elif col['type'] == 'cluster':
                                clusters = (c_anoms.Time.diff() > HOUR).sum() + 1
                                X[col['name']].append(clusters)
                            elif col['type'] == 'sequence':
                                X[col['name']].append(
                                    (t - c_anoms.Time).dt.days.tolist())
                            else:
                                X[col['name']].append(1)

                        elif col['type'] == 'special':
                            # Special variables with named logic
                            if col['name'] in deprecated:
                                X[col['name']].append(0)
                            elif 'TRIPLE_THREAT' in col['name']:
                                c_anoms = self._triple_threat(f_anoms)
                                min_lag = t - pd.Timedelta(hours=col['min_lag'])
                                max_lag = t - pd.Timedelta(hours=col['max_lag'])
                                c_anoms = c_anoms.loc[(c_anoms.Time <= min_lag) &
                                                      (c_anoms.Time > max_lag)]
                                X[col['name']].append(c_anoms.shape[0])
                            else:
                                raise ValueError('Unknown special column %s.' %
                                                 col['name'])

                        elif col['type'] in {'duration', 'constant'}:
                            # These are more efficient to map once dataset is
                            # built
                            X[col['name']].append(0)

                        else:
                            raise ValueError('Unknown column type %s.' %
                                             col['type'])

        X = pd.DataFrame(X)
        X = X[col_order]
        y = pd.DataFrame(y)

        for col in constants:
            const_mapper = {f: self.feeder_df.loc[f, col] for f in
                            self.feeder_df.index}
            X[col] = y.FEEDER.map(const_mapper)

        ns_to_years = 1e9 * 60 * 60 * 24 * 365.

        for col in durations:
            # Calculate the number of years since an event
            const_mapper = {f: self.feeder_df.loc[f, col] for f in
                            self.feeder_df.index}
            X[col] = y.FEEDER.map(const_mapper)
            if not X.empty and not y.empty:
                X[col] = y.TIMESTAMP - X[col]
                X[col] = X[col].map(lambda t: np.floor(int(t) / ns_to_years))

        self.X = X
        self.y = y

        return self

    def _predict(self):
        """Create predictions for a set of anomaly signatures."""
        # Roll through all combinations of the deprecated binary variables
        i = 0
        X = self.X.copy()
        y = self.y.copy()
        preds = None
        deprecated = ['FDR_GEO_0', 'FDR_GEO_1', 'FDR_GEO_2',
                      'IS_DADE', 'SIG_OUTLIER']
        if all([col in X.columns for col in deprecated]):
            for is_dade in [0, 1]:
                for sig_outlier in [0, 1]:
                    for fdr_geo in ['FDR_GEO_0', 'FDR_GEO_1', 'FDR_GEO_2']:
                        X.IS_DADE = is_dade
                        X.SIG_OUTLIER = sig_outlier
                        X.FDR_GEO_0 = 0
                        X.FDR_GEO_1 = 0
                        X.FDR_GEO_2 = 0
                        X[fdr_geo] = 1
                        if not X.empty:
                            preds = self.clf.predict_proba(X.values)[:, 1]
                        y['PROB_' + str(i)] = preds
                        i += 1
        elif not X.empty:
            y['PROB'] = self.clf.predict_proba(X.values)[:, 1]
        # Use the maximum predicted probability
        y = y.set_index(['FEEDER', 'TIMESTAMP'])
        y = pd.DataFrame(y.max(axis=1))
        y.columns = ['PROB']
        y = y.reset_index()

        # Set alert thresholds and names
        y['ALERT'] = ' '
        y['TIER'] = 0.
        lower = 0.0
        for i, upper in enumerate(self.alert_config['Value']):
            y.loc[(y.PROB >= lower) & (y.PROB < upper), 'TIER'] = lower
            y.loc[(y.PROB >= lower) & (y.PROB < upper),
                  'ALERT'] = self.alert_config['Name'][i]
            lower = upper

        self.preds = y

        return self

    def _generate_alerts(self, unprocessed, processed, old_preds):
        """Generate alerts based on the predictions.

        Parameters
        ----------
        unprocessed : pd.DataFrame
            DataFrame containing one row per anomaly. Same format as the output
            of the anomaly module. These are the new, previously unprocessed
            anomalies.

        processed : pd.DataFrame, or None
            DataFrame containing one row per anomaly. Same format as the output
            of the anomaly module. These are the previously processed anomalies
            from older evaluation jobs. Use None or an empty DataFrame to
            represent the case of turning on the system where there is no old
            previously processed data.

        old_preds : pd.DataFrame
            DataFrame containing one row per signature. Same format as the
            ``preds`` attribute of this class. These are the results yielded by
            previously processed anomalies from older evaluation jobs.
        """
        if not hasattr(self, 'preds'):
            raise ValueError("Run _predict before creating alerts.")

        feeders = self.preds.loc[self.preds.ALERT != 'None', 'FEEDER'].unique()

        # Collate & clean anomalies
        use_processed = True
        if processed is None or processed.empty:
            use_processed = False
        if use_processed:
            processed = self._clean_data(processed, feeders)
            if processed.empty:
                use_processed = False
        anomalies = self._clean_data(unprocessed, feeders)
        if use_processed:
            anomalies = pd.concat([anomalies, processed])

        # AMI last gasps / power downs can come in slowly over time.
        # This results in duplicate anomalies that need to be dropped.
        # Sorting by "Signal" will keep the highest anomaly only.
        anomalies = anomalies.sort_values('Signal', ascending=False)
        dup_cols = ['Anomaly', 'Feeder', 'Time']
        anomalies = anomalies.loc[(anomalies.DeviceType != 'AMI') |
                                  (~anomalies[dup_cols].duplicated())]

        self.alerts = []

        for f in sorted(feeders):
            # Get all the feeder meta data we need for reporting
            meta_cols = ['CEMM35_FEEDER', '4N+_FEEDER', 'FEEDER', 'REGION',
                         'AREA', 'SUBSTATION', 'KV', 'CUSTOMERS', 'FDR_OH',
                         'FDR_UG', 'LAT_OH', 'LAT_UG', 'HARDENING']
            f_df = self.feeder_df.loc[f, meta_cols].copy()
            if f_df.loc['HARDENING'] < pd.Timestamp('1985-01-01', tz=UTC):
                f_df.loc['HARDENING'] = "None"
            else:
                f_df.loc['HARDENING'] = f_df.loc['HARDENING'].date().isoformat()
            f_df.loc['CEMM35_FEEDER'] = f_df.loc['CEMM35_FEEDER'].astype(str)
            f_df.loc['4N+_FEEDER'] = f_df.loc['4N+_FEEDER'].astype(str)
            for col in ['FDR_OH', 'FDR_UG', 'LAT_OH', 'LAT_UG']:
                f_df.loc[col] = "%.1f mi" % f_df.loc[col]
            f_df.index = ['CEMM25 Feeder', '4N+ Feeder', 'Feeder', 'Region',
                          'Area', 'Substation', 'KV', 'Customers', 'Feeder OH',
                          'Feeder UG', 'Lateral OH', 'Lateral UG', 'Hardening']

            f_alerts = self.preds.loc[(self.preds.FEEDER == f) &
                                      (self.preds.ALERT != 'None')]
            f_alerts = f_alerts.sort_values(by='TIMESTAMP')
            for t in f_alerts.TIMESTAMP.tolist():
                # We only issue alerts if the current signature has achieved a
                # new "high water mark" for that feeder for the prior 24 hours.
                # eg. it has gone from red to orange.
                this_alert = f_alerts.loc[f_alerts.TIMESTAMP == t]
                prior_alerts = f_alerts.loc[(f_alerts.TIMESTAMP < t) &
                                            (f_alerts.TIMESTAMP >= t - DAY)]
                if prior_alerts.empty:
                    prior_alerts = 0.
                else:
                    prior_alerts = prior_alerts.TIER.max()
                current_alert = this_alert.TIER.values[0]
                if current_alert > prior_alerts:
                    # If we previously sent alerts for this feeder, skip it.
                    if old_preds is not None and not old_preds.empty:
                        if not old_preds.loc[(old_preds.TIER >= current_alert) &
                                             (old_preds.TIMESTAMP < t + DAY) &
                                             (old_preds.TIMESTAMP >= t - DAY) &
                                             (old_preds.FEEDER == f)].empty:
                            continue
                    t = this_alert.TIMESTAMP.tolist()[0]
                    # Now collect the raw anomalies that went into the signature
                    f_anoms = []
                    for col in self.dataset_config:
                        if col['type'] in {'trigger', 'flag', 'background'}:
                            c_name = col['lookup']
                            min_lag = t - pd.Timedelta(hours=col['min_lag'])
                            max_lag = t - pd.Timedelta(hours=col['max_lag'])
                            c_anoms = anomalies.loc[(anomalies.Feeder == f) &
                                                    (anomalies.Anomaly == c_name) &
                                                    (anomalies.Time <= min_lag) &
                                                    (anomalies.Time > max_lag)].copy()
                            if not c_anoms.empty:
                                f_anoms.append(c_anoms)

                    f_anoms = pd.concat(f_anoms)
                    f_anoms = f_anoms.reset_index(drop=True)

                    self.alerts.append(_Alert(feeder_id=f,
                                              timestamp=t,
                                              alert_type=this_alert.ALERT.values[0],
                                              model_id=self.model_id,
                                              anomalies=f_anoms,
                                              feeder_metadata=f_df))

        return self

    def generate_alerts(self, unprocessed, processed, old_alerts):
        """Generate alerts and anomaly data based on the predictions.

        Parameters
        ----------
        unprocessed : pd.DataFrame
            DataFrame containing one row per anomaly. Same format as the output
            of the anomaly module. These are the new, previously unprocessed
            anomalies.

        processed : pd.DataFrame, or None
            DataFrame containing one row per anomaly. Same format as the output
            of the anomaly module. These are the previously processed anomalies
            from older evaluation jobs. Use None or an empty DataFrame to
            represent the case of turning on the system where there is no old
            previously processed data.

        old_preds : pd.DataFrame
            DataFrame containing one row per signature. Same format as the
            ``preds`` attribute of this class. These are the results yielded by
            previously processed anomalies from older evaluation jobs.
        """
        self._transform(unprocessed, processed)
        self._predict()
        self._generate_alerts(unprocessed, processed, old_alerts)

        return self

    def __len__(self):
        """Overload `len` output to be the number of alerts generated."""
        if not hasattr(self, "alerts"):
            return 0
        return len(self.alerts)

    def __getitem__(self, item):
        """Return the ith alert of the alerts generated."""
        if item >= len(self):
            raise IndexError
        return self.alerts[item]
